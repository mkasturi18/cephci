# This test to verify upgrade with archive zone with multisite from 5.x(latest) to 5.3 with SSL
# conf: rgw_ms_archive.yaml
# Polarion ID: CEPH-83574647
# platform: rhel-9

tests:

#  # Cluster deployment stage
#  - test:
#      abort-on-fail: true
#      desc: Install software pre-requisites for cluster deployment.
#      module: install_prereq.py
#      name: setup pre-requisites
#
#  - test:
#      abort-on-fail: true
#      clusters:
#        ceph-pri:
#          config:
#            verify_cluster_health: true
#            steps:
#              - config:
#                  command: bootstrap
#                  service: cephadm
#                  args:
#                    rhcs-version: 5.3
#                    release: ga
#                    registry-url: registry.redhat.io
#                    mon-ip: node1
#                    orphan-initial-daemons: true
#                    initial-dashboard-password: admin@123
#                    dashboard-password-noupdate: true
#              - config:
#                  command: add_hosts
#                  service: host
#                  args:
#                    attach_ip_address: true
#                    labels: apply-all-labels
#              - config:
#                  command: apply
#                  service: mgr
#                  args:
#                    placement:
#                      label: mgr
#              - config:
#                  command: apply
#                  service: mon
#                  args:
#                    placement:
#                      label: mon
#              - config:
#                  command: apply
#                  service: osd
#                  args:
#                    all-available-devices: true
#              - config:
#                  command: apply_spec
#                  service: orch
#                  specs:
#                    - service_type: rgw
#                      service_id: shared.pri
#                      spec:
#                        ssl: true
#                        rgw_frontend_ssl_certificate: create-cert
#                      placement:
#                        nodes:
#                          - node5
#        ceph-sec:
#          config:
#            verify_cluster_health: true
#            steps:
#              - config:
#                  command: bootstrap
#                  service: cephadm
#                  args:
#                    rhcs-version: 5.3
#                    release: ga
#                    registry-url: registry.redhat.io
#                    mon-ip: node1
#                    orphan-initial-daemons: true
#                    initial-dashboard-password: admin@123
#                    dashboard-password-noupdate: true
#              - config:
#                  command: add_hosts
#                  service: host
#                  args:
#                    attach_ip_address: true
#                    labels: apply-all-labels
#              - config:
#                  command: apply
#                  service: mgr
#                  args:
#                    placement:
#                      label: mgr
#              - config:
#                  command: apply
#                  service: mon
#                  args:
#                    placement:
#                      label: mon
#              - config:
#                  command: apply
#                  service: osd
#                  args:
#                    all-available-devices: true
#              - config:
#                  command: apply_spec
#                  service: orch
#                  specs:
#                    - service_type: rgw
#                      service_id: shared.sec
#                      spec:
#                        ssl: true
#                        rgw_frontend_ssl_certificate: create-cert
#                      placement:
#                        nodes:
#                          - node5
#        ceph-arc:
#          config:
#            verify_cluster_health: true
#            steps:
#              - config:
#                  command: bootstrap
#                  service: cephadm
#                  args:
#                    rhcs-version: 5.3
#                    release: ga
#                    registry-url: registry.redhat.io
#                    mon-ip: node1
#                    orphan-initial-daemons: true
#                    initial-dashboard-password: admin@123
#                    dashboard-password-noupdate: true
#              - config:
#                  command: add_hosts
#                  service: host
#                  args:
#                    attach_ip_address: true
#                    labels: apply-all-labels
#              - config:
#                  command: apply
#                  service: mgr
#                  args:
#                    placement:
#                      label: mgr
#              - config:
#                  command: apply
#                  service: mon
#                  args:
#                    placement:
#                      label: mon
#              - config:
#                  command: apply
#                  service: osd
#                  args:
#                    all-available-devices: true
#              - config:
#                  command: apply_spec
#                  service: orch
#                  specs:
#                    - service_type: rgw
#                      service_id: shared.arc
#                      spec:
#                        ssl: true
#                        rgw_frontend_ssl_certificate: create-cert
#                      placement:
#                        nodes:
#                          - node5
#      desc: RHCS cluster deployment using cephadm.
#      polarion-id: CEPH-83575222
#      destroy-cluster: false
#      module: test_cephadm.py
#      name: deploy cluster
#  - test:
#      abort-on-fail: true
#      clusters:
#        ceph-pri:
#          config:
#            verify_cluster_health: true
#            steps:
#              - config:
#                  command: apply_spec
#                  service: orch
#                  validate-spec-services: true
#                  specs:
#                    - service_type: prometheus
#                      placement:
#                        count: 1
#                        nodes:
#                          - node1
#                    - service_type: grafana
#                      placement:
#                        nodes:
#                          - node1
#                    - service_type: alertmanager
#                      placement:
#                        count: 1
#                    - service_type: node-exporter
#                      placement:
#                        host_pattern: "*"
#                    - service_type: crash
#                      placement:
#                        host_pattern: "*"
#        ceph-sec:
#          config:
#            verify_cluster_health: true
#            steps:
#              - config:
#                  command: apply_spec
#                  service: orch
#                  validate-spec-services: true
#                  specs:
#                    - service_type: prometheus
#                      placement:
#                        count: 1
#                        nodes:
#                          - node1
#                    - service_type: grafana
#                      placement:
#                        nodes:
#                          - node1
#                    - service_type: alertmanager
#                      placement:
#                        count: 1
#                    - service_type: node-exporter
#                      placement:
#                        host_pattern: "*"
#                    - service_type: crash
#                      placement:
#                        host_pattern: "*"
#        ceph-arc:
#          config:
#            verify_cluster_health: true
#            steps:
#              - config:
#                  command: apply_spec
#                  service: orch
#                  validate-spec-services: true
#                  specs:
#                    - service_type: prometheus
#                      placement:
#                        count: 1
#                        nodes:
#                          - node1
#                    - service_type: grafana
#                      placement:
#                        nodes:
#                          - node1
#                    - service_type: alertmanager
#                      placement:
#                        count: 1
#                    - service_type: node-exporter
#                      placement:
#                        host_pattern: "*"
#                    - service_type: crash
#                      placement:
#                        host_pattern: "*"
#      name: Monitoring Services deployment
#      desc: Add monitoring services using spec file.
#      module: test_cephadm.py
#      polarion-id: CEPH-83574727
#  - test:
#      abort-on-fail: true
#      clusters:
#        ceph-pri:
#          config:
#            command: add
#            id: client.1
#            node: node6
#            install_packages:
#              - ceph-common
#            copy_admin_keyring: true
#        ceph-sec:
#          config:
#            command: add
#            id: client.1
#            node: node6
#            install_packages:
#              - ceph-common
#            copy_admin_keyring: true
#        ceph-arc:
#          config:
#            command: add
#            id: client.1
#            node: node6
#            install_packages:
#              - ceph-common
#            copy_admin_keyring: true
#      desc: Configure the RGW client system
#      polarion-id: CEPH-83573758
#      destroy-cluster: false
#      module: test_client.py
#      name: configure client
  - test:
      abort-on-fail: true
      clusters:
        ceph-pri:
          config:
            cephadm: true
            commands:
              - "radosgw-admin realm create --rgw-realm india --default"
              - "radosgw-admin zonegroup create --rgw-realm india --rgw-zonegroup shared --endpoints https://{node_ip:node5}:443 --master --default"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --endpoints https://{node_ip:node5}:443 --master --default"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "radosgw-admin user create --uid=repuser --display_name='Replication user' --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --rgw-realm india --system"
              - "radosgw-admin zone modify --rgw-realm india --rgw-zonegroup shared --rgw-zone primary --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_zone primary"
              - "ceph config set client.rgw rgw_verify_ssl False"
              - "ceph config set client.rgw.{daemon_id:shared.pri} rgw_verify_ssl False"
              - "ceph orch restart {service_name:shared.pri}"
        ceph-sec:
          config:
            cephadm: true
            commands:
              - "sleep 120"
              - "radosgw-admin realm pull --rgw-realm india --url https://{node_ip:ceph-pri#node5}:443 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url https://{node_ip:ceph-pri#node5}:443 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone secondary --endpoints https://{node_ip:node5}:443 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.sec} rgw_zone secondary"
              - "ceph orch restart {service_name:shared.sec}"
        ceph-arc:
          config:
            cephadm: true
            commands:
              - "sleep 200"
              - "radosgw-admin realm pull --rgw-realm india --url https://{node_ip:ceph-pri#node5}:443 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --default"
              - "radosgw-admin period pull --url https://{node_ip:ceph-pri#node5}:443 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d"
              - "radosgw-admin zone create --rgw-realm india --rgw-zonegroup shared --rgw-zone archive --endpoints https://{node_ip:node5}:443 --access-key 21e86bce636c3aa0 --secret cf764951f1fdde5d --tier-type=archive"
              - "radosgw-admin period update --rgw-realm india --commit"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_realm india"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_zonegroup shared"
              - "ceph config set client.rgw.{daemon_id:shared.arc} rgw_zone archive"
              - "ceph orch restart {service_name:shared.arc}"
      desc: Setting up RGW multisite replication environment with archive zone
      module: exec.py
      name: setup multisite
      polarion-id: CEPH-83575393

  - test:
      clusters:
        ceph-pri:
          config:
            set-env: true
            script-name: user_create.py
            config-file-name: non_tenanted_user.yaml
            copy-user-info-to-site: ceph-arc
            timeout: 300
      desc: create non-tenanted user
      module: sanity_rgw_multisite.py
      name: create non-tenanted user
      polarion-id: CEPH-83575199

  - test:
      name: listing flat ordered versioned buckets on primary
      desc: test_bucket_listing_flat_ordered_versionsing on secondary
      polarion-id: CEPH-83573545
      module: sanity_rgw_multisite.py
      clusters:
        ceph-pri:
          config:
            script-name: test_bucket_listing.py
            config-file-name: test_bucket_listing_flat_ordered_versionsing.yaml
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
            timeout: 300

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dynamic_resharding_brownfield.yaml
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
            timeout: 300
      desc: Create bucket to test for Testing dynamic resharding brownfield scenario after upgrade
      module: sanity_rgw_multisite.py
      name: Create bucket for Testing dynamic resharding brownfield scenario after upgrade
      polarion-id: CEPH-83574736

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_manual_resharding_brownfield.yaml
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
            timeout: 300
      desc: Create bucket for Testing manual resharding brownfield scenario after upgrade
      module: sanity_rgw_multisite.py
      name: Create bucket for Testing manual resharding brownfield scenario after upgrade
      polarion-id: CEPH-83574735

  - test:
      abort-on-fail: true
      desc: Multisite upgrade with archive cluster
      module: test_cephadm_upgrade.py
      name: multisite ceph upgrade
      polarion-id: CEPH-83574647
      clusters:
        ceph-pri:
          config:
            command: start
            service: upgrade
            verify_cluster_health: true
        ceph-sec:
          config:
            command: start
            service: upgrade
            verify_cluster_health: true
        ceph-arc:
          config:
            command: start
            service: upgrade
            verify_cluster_health: true

  - test:
      desc: Retrieve the versions of the cluster
      module: exec.py
      name: post upgrade gather version
      polarion-id: CEPH-83575200
      config:
        cephadm: true
        commands:
          - "ceph versions"
  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dynamic_resharding_brownfield.yaml
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
            timeout: 300
      desc: Test dynamic resharding brownfield scenario after upgrade
      module: sanity_rgw_multisite.py
      name: Test dynamic resharding brownfield scenario after upgrade
      polarion-id: CEPH-83574736

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_manual_resharding_brownfield.yaml
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
            timeout: 300
      desc: Test manual resharding brownfield scenario after upgrade
      module: sanity_rgw_multisite.py
      name: Test dynamic resharding brownfield scenario after upgrade
      polarion-id: CEPH-83574735

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_dynamic_resharding_greenfield.yaml
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
            timeout: 300
      desc: Test dynamic resharding brownfield scenario after upgrade on new bucket
      abort-on-fail: true
      module: sanity_rgw_multisite.py
      name: Dynamic Resharding tests on Primary cluster
      polarion-id: CEPH-83574737

  - test:
      clusters:
        ceph-pri:
          config:
            script-name: test_Mbuckets_with_Nobjects.py
            config-file-name: test_multisite_manual_resharding_greenfield.yaml
            verify-io-on-site: ["ceph-sec", "ceph-arc"]
            timeout: 300
      desc: Test manual resharding brownfield scenario after upgrade on new bucket
      abort-on-fail: true
      module: sanity_rgw_multisite.py
      name: Manual Resharding tests on Primary cluster
      polarion-id: CEPH-83574734
