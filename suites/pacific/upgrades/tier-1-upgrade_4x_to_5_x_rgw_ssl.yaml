tests:
- test:
    name: install ceph pre-requisites
    module: install_prereq.py
    abort-on-fail: True

- test:
    name: ceph ansible install rhcs 4.x from cdn
    polarion-id: CEPH-83573588
    module: test_ansible.py
    config:
      use_cdn: True
      build: '4.x'
      cmd: openssl req -x509 -newkey rsa:2048 -keyout server.key -out server.crt -days 365 -nodes -subj "/C=IN/ST=KA/L=BLR/O=Carina Company/OU=Redhat/CN=*.ceph.redhat.com"
      cmd: cat server.crt server.key > /etc/ssl/certs/server.pem
      cmd: cp server.crt /etc/pki/ca-trust/source/anchors/
      cmd: update-ca-trust extract
      sudo: True
      ansi_config:
        ceph_origin: repository
        ceph_repository: rhcs
        ceph_repository_type: cdn
        ceph_rhcs_version: 4
        ceph_stable_release: nautilus
        osd_scenario: lvm
        osd_auto_discovery: False
        ceph_stable_rh_storage: True
        ceph_docker_image: "rhceph/rhceph-4-rhel8"
        ceph_docker_image_tag: "latest"
        ceph_docker_registry: "registry.redhat.io"
        copy_admin_key: true
        radosgw_frontend_ssl_certificate: "/etc/ssl/certs/server.pem"
        dashboard_admin_user: admin
        dashboard_admin_password: p@ssw0rd
        grafana_admin_user: admin
        grafana_admin_password: p@ssw0rd
        node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6
        grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
        prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:v4.6
        alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6
        ceph_conf_overrides:
            global:
              osd_pool_default_pg_num: 64
              osd_default_pool_size: 2
              osd_pool_default_pgp_num: 64
              mon_max_pg_per_osd: 1024
            mon:
              mon_allow_pool_delete: true
            client:
              rgw crypt require ssl: false
              rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
        cephfs_pools:
          - name: "cephfs_data"
            pgs: "8"
          - name: "cephfs_metadata"
            pgs: "8"
    desc: deploy ceph containerized 4.x cdn setup using ceph-ansible
    destroy-cluster: False
    abort-on-fail: true

- test:
    name: check-ceph-health
    module: exec.py
    config:
      cmd: ceph -s
      cmd: ceph versions
      cmd: radosgw-admin sync status
      sudo: True
    desc: Check for ceph health debug info

    #- test:
    #name: Beast SSL RGW test
    #desc: RGW SSL testing with Beast frontend enabled
    #polarion-id: CEPH-10359
    #module: sanity_rgw.py
    #config:
    #  test-version: v2
    #  script-name: test_frontends_with_ssl.py
    #  config-file-name: test_ssl_beast.yaml
    # timeout: 500

- test:
    name: Mbuckets
    desc: test to create "M" no of buckets
    polarion-id: CEPH-9789
    module: sanity_rgw.py
    config:
      script-name: test_Mbuckets_with_Nobjects.py
      config-file-name: test_Mbuckets.yaml
      timeout: 300
- test:
    name: Mbuckets_with_Nobjects
    desc: test to create "M" no of buckets and "N" no of objects
    polarion-id: CEPH-9789
    module: sanity_rgw.py
    config:
      script-name: test_Mbuckets_with_Nobjects.py
      config-file-name: test_Mbuckets_with_Nobjects.yaml
      timeout: 300
- test:
    config:
      script-name: test_Mbuckets_with_Nobjects.py
      config-file-name: test_Mbuckets_with_Nobjects_delete.yaml
      timeout: 300
    desc: test to create "M" no of buckets and "N" no of objects with delete
    module: sanity_rgw.py
    name: Test delete using M buckets with N objects
    polarion-id: CEPH-14237
- test:
      name: switch-from-non-containerized-to-containerized-ceph-daemons
      polarion-id: CEPH-83573510
      module: switch_rpm_to_container.py
      abort-on-fail: true

- test:
    name: check-ceph-health
    module: exec.py
    config:
      cmd: ceph -s
      cmd: ceph versions
      cmd: radosgw-admin sync status
      sudo: True
    desc: Check for ceph health debug info

- test:
    name: Upgrade containerized ceph to 5.x latest
    polarion-id: CEPH-83573679
    module: test_ansible_upgrade.py
    config:
      ansi_config:
        ceph_origin: distro
        ceph_repository: rhcs
        ceph_rhcs_version: 5
        osd_scenario: lvm
        osd_auto_discovery: False
        ceph_stable: True
        ceph_stable_rh_storage: True
        fetch_directory: ~/fetch
        copy_admin_key: true
        containerized_deployment: true
        upgrade_ceph_packages: True
        radosgw_frontend_ssl_certificate: "/etc/ssl/certs/server.pem"
        dashboard_admin_user: admin
        dashboard_admin_password: p@ssw0rd
        grafana_admin_user: admin
        grafana_admin_password: p@ssw0rd
        ceph_docker_image: "rh-osbs/rhceph" #"rhceph/rhceph-5-rhel8"
        ceph_docker_image_tag: "ceph-5.0-rhel-8-containers-candidate-33758-20210727010426" #"latest"
        ceph_docker_registry: "registry-proxy.engineering.redhat.com" #"registry.redhat.io"
        node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6
        grafana_container_image: registry.redhat.io/rhceph-beta/rhceph-5-dashboard-rhel8:latest #registry-proxy.engineering.redhat.com/rh-osbs/grafana:5-26
        prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:v4.6
        alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6
        ceph_conf_overrides:
            global:
              osd_pool_default_pg_num: 64
              osd_default_pool_size: 2
              osd_pool_default_pgp_num: 64
              mon_max_pg_per_osd: 1024
            mon:
              mon_allow_pool_delete: true
            client:
              rgw crypt require ssl: false
              rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
    desc: Test Ceph-Ansible rolling update 5.x cdn -> 5.x latest
    abort-on-fail: True

- test:
    name: check-ceph-health
    module: exec.py
    config:
      cmd: ceph -s
      cmd: ceph versions
      cmd: radosgw-admin sync status
      sudo: True
    desc: Check for ceph health debug info

- test:
    name: Mbuckets
    desc: test to create "M" no of buckets
    polarion-id: CEPH-9789
    module: sanity_rgw.py
    config:
      script-name: test_Mbuckets_with_Nobjects.py
      config-file-name: test_Mbuckets.yaml
      timeout: 300

- test:
    name: Mbuckets_with_Nobjects
    desc: test to create "M" no of buckets and "N" no of objects with encryption
    module: sanity_rgw.py
    config:
      script-name: test_Mbuckets_with_Nobjects.py
      config-file-name: test_Mbuckets_with_Nobjects_compression.yaml
      timeout: 300
